import numpy as np
from numpy import random
from game.game import Game

class Full_Information_Game(Game):
    def __init__(self, data_generating_mechanism, bandit_algorithm, store_rewards = False):
        super().__init__(data_generating_mechanism, bandit_algorithm)
        self.store_rewards = store_rewards
        
    def simulate_one_run(self):
       history = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                   self.data_generating_mechanism.get_T()))
       current_loss = np.zeros(shape = self.data_generating_mechanism.get_T())
       regret = np.zeros(shape = self.data_generating_mechanism.get_T())
       
       for t in range(1, self.data_generating_mechanism.get_T() + 1):
            # estimate arm values 
            I_t = self.bandit_algorithm.get_arm_to_pull(history, t, self.store_rewards)

            # reward is  
            # generated by
            # nature
            r_t = self.data_generating_mechanism.get_rewards(t)

            # Update the statistics
            for i in range(self.data_generating_mechanism.get_K()): 
                history[i][t-1] = r_t[i] if self.store_rewards else 1 - r_t[i]
            
            current_loss[t-1] = r_t[i] if self.store_rewards else 1 - r_t[I_t]
            
       cumulative_losses = np.sum(history, axis=1)
       optimal_arm = np.argmax(cumulative_losses) if self.store_rewards else np.argmin(cumulative_losses)

       multiplier = -1 if self.store_rewards else 1
       for t in range(self.data_generating_mechanism.get_T()):
        if (t == 0):
            regret[t] = (current_loss[t] - history[optimal_arm][t]) * multiplier
        else: 
            regret[t] = regret[t-1] + ((current_loss[t] - history[optimal_arm][t]) * multiplier)

       return regret


        