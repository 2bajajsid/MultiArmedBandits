import numpy as np
from numpy import random
from game.game import Game
import matplotlib.pyplot as plt
from scipy.optimize import minimize_scalar, Bounds, minimize
from bayes_opt import BayesianOptimization, acquisition, SequentialDomainReductionTransformer

class Partial_Information_Game(Game):
    def __init__(self, bandit_algorithm):
        super().__init__(bandit_algorithm)

    def simulate_one_run(self, hyperparameters):
        # initialize the data structures
        # to hold the losses of each arm
        self.bandit_algorithm.data_generating_mechanism.initialize_parameters(hyperparameters)
        importance_weighted_history = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                                        self.data_generating_mechanism.get_T()))
        
        unweighted_history = []
        full_history = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                         self.data_generating_mechanism.get_T()))
        arm_chosen = np.zeros(shape = self.data_generating_mechanism.get_T())
        regret = np.zeros(shape = self.data_generating_mechanism.get_T())

        for i in range(self.data_generating_mechanism.get_K()):
            unweighted_history.append([])

        for t in range(self.data_generating_mechanism.get_T()):
            # estimate arm values 
            I_t = self.bandit_algorithm.get_arm_to_pull(t)

            # reward is  
            # generated by
            # nature
            r_t = self.bandit_algorithm.data_generating_mechanism.get_rewards(t)
            if (self.bandit_algorithm.data_generating_mechanism.mustUpdateStatistics):
                self.bandit_algorithm.data_generating_mechanism.update_statistics(I_t, r_t[I_t], t)

            # update the statistics
            # only add non-zero value to the history
            # if arm was chosen by the learner
            # to simulate partial info
            for i in range(self.data_generating_mechanism.get_K()): 
                if (i == I_t):
                    unweighted_history[i].append(r_t[i])
                    arm_chosen[t] = int(i)
                else:
                    importance_weighted_history[i][t] = 0
                full_history[i][t] = r_t[i]
            
        cumulative_rewards = np.sum(full_history, axis=1)
        optimal_arm_mean = self.bandit_algorithm.data_generating_mechanism.get_optimal_arm_mean()

        for t in range(self.bandit_algorithm.data_generating_mechanism.get_T()):
            if (t == 0):
                regret[t] = optimal_arm_mean - self.bandit_algorithm.data_generating_mechanism.get_arm_mean(arm_chosen[t])
            else: 
                regret[t] = regret[t-1] + (optimal_arm_mean - self.bandit_algorithm.data_generating_mechanism.get_arm_mean(arm_chosen[t]))

        return regret
    
    def simulate_all_runs(self, hyperparameters):
        print("Simulating with parameter {}".format(hyperparameters))
        num_runs = self.data_generating_mechanism.get_M()
        final_regret = np.zeros(shape = num_runs)
        for i in range(0, num_runs):
            if i % 5000 == 0:
                print('m = {:d}'.format(i))
            final_regret[i] = self.simulate_one_run(hyperparameters)[self.data_generating_mechanism.get_T() - 1]
        self.compute_regret_sub()
        return self.get_averaged_regret() + (1.96 * np.std(self.regret_sub_mean) / np.sqrt(self.data_generating_mechanism.prior_samples))
    
    def wrapper(self, **hyperparameters):
        self.simulate_all_runs(hyperparameters)
        self.compute_regret_sub()
        return -1 * self.get_averaged_regret()
    
    def find_minimum(self, bounds=(0,1), bracket=(0.0001, 0.01, 0.99)):
        results = dict()
        results = minimize_scalar(self.compute_averaged_regret,  
                           bounds=(0,1),
                           method='bounded')
        print(results)
        '''
        results = minimize_scalar(self.simulate_all_runs, bounds = (0, 1))
        minimium_window = {'lambda': 0.25, 'delta': 0.025}
        pbounds = {'lambda': (0.0001, 5), 'delta': (0.0000001, 0.1)}
        minimium_window = {'delta': 0.025}
        pbounds = {'delta': (0.0000001, 0.1)}
        xi=0.0025
        minimium_window = {'alpha': 0.025}
        pbounds = {'alpha': (0.000001, 1)}
        bounds_transformer = SequentialDomainReductionTransformer(minimum_window=minimium_window)
        xi=0.025
        optimizer = BayesianOptimization(
            f = self.wrapper,
            pbounds = pbounds,
            random_state=0,
            acquisition_function= acquisition.ProbabilityOfImprovement(xi=xi),
            verbose=2,
            bounds_transformer=bounds_transformer
        )
        optimizer.maximize(init_points = 3, n_iter = 25)
        print(optimizer.max)
        '''

    def grid_search(self, grid_values):
        print(np.shape(grid_values))
        y = np.zeros(shape = len(grid_values))
        for i in range(len(grid_values)):
            print("In Grid Value {}".format(i))
            y[i] = self.simulate_all_runs(grid_values[i])
            print("Finished grid value {} with final averaged regret {}".format(grid_values[i], y[i]))

        plt.legend()
        plt.plot(grid_values, y)
        plt.title("Bayesian Regret as a function of lambda")
        plt.ylabel('Bayesian Regret')
        plt.xlabel('Lambda')
        plt.show()




    
        
    