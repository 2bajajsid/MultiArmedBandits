import numpy as np
from numpy import random
from game.game import Game
import matplotlib.pyplot as plt
from scipy.optimize import minimize_scalar, Bounds, minimize
from bayes_opt import BayesianOptimization

class Partial_Information_Game(Game):
    def __init__(self, bandit_algorithm):
        super().__init__(bandit_algorithm)

    def simulate_one_run(self, hyperparameters):
        # initialize the data structures
        # to hold the losses of each arm
        self.bandit_algorithm.data_generating_mechanism.initialize_parameters(hyperparameters)
        importance_weighted_history = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                                        self.data_generating_mechanism.get_T()))
        
        unweighted_history = []
        full_history = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                         self.data_generating_mechanism.get_T()))
        arm_chosen = np.zeros(shape = self.data_generating_mechanism.get_T())
        regret = np.zeros(shape = self.data_generating_mechanism.get_T())

        for i in range(self.data_generating_mechanism.get_K()):
            unweighted_history.append([])

        for t in range(self.data_generating_mechanism.get_T()):
            # estimate arm values 
            I_t = self.bandit_algorithm.get_arm_to_pull(t)

            # reward is  
            # generated by
            # nature
            r_t = self.bandit_algorithm.data_generating_mechanism.get_rewards(t)
            if (self.bandit_algorithm.data_generating_mechanism.mustUpdateStatistics):
                self.bandit_algorithm.data_generating_mechanism.update_statistics(I_t, r_t[I_t], t)

            # update the statistics
            # only add non-zero value to the history
            # if arm was chosen by the learner
            # to simulate partial info
            for i in range(self.data_generating_mechanism.get_K()): 
                if (i == I_t):
                    unweighted_history[i].append(r_t[i])
                    arm_chosen[t] = int(i)
                else:
                    importance_weighted_history[i][t] = 0
                full_history[i][t] = r_t[i]
            
        cumulative_rewards = np.sum(full_history, axis=1)
        optimal_arm_mean = self.bandit_algorithm.data_generating_mechanism.get_optimal_arm_mean()

        for t in range(self.bandit_algorithm.data_generating_mechanism.get_T()):
            if (t == 0):
                regret[t] = optimal_arm_mean - self.bandit_algorithm.data_generating_mechanism.get_arm_mean(arm_chosen[t])
            else: 
                regret[t] = regret[t-1] + (optimal_arm_mean - self.bandit_algorithm.data_generating_mechanism.get_arm_mean(arm_chosen[t]))

        return regret
    
    def simulate_all_runs(self, hyperparameters):
        print("Simulating with parameter {}".format(hyperparameters))
        num_runs = self.data_generating_mechanism.get_M()
        final_regret = np.zeros(shape = num_runs)
        for i in range(0, num_runs):
            final_regret[i] = self.simulate_one_run(hyperparameters)[self.data_generating_mechanism.get_T() - 1]
        ave_regret = np.average(final_regret)
        print(ave_regret)
        return ave_regret
    
    def wrapper(self, **hyperparameters):
        return -1 * self.simulate_all_runs(hyperparameters)
    
    def find_minimum(self, starting_values, bounds):
        results = dict()
        '''
        results = minimize(self.simulate_all_runs, 
                           method='Nelder-Mead', 
                           x0 = starting_values, 
                           bounds=bounds)
        results = minimize_scalar(self.simulate_all_runs, bounds = (0, 1))
        '''
        optimizer = BayesianOptimization(
            f = self.wrapper,
            pbounds = {'lambda': (0.0001, 5), 
                       'delta': (0.0000001, 0.1)},
            random_state=0,
            verbose=0
        )
        optimizer.maximize(init_points = 2, n_iter = 25)
        print(optimizer.max)

    def grid_search(self, grid_values):
        print(np.shape(grid_values))
        y = np.zeros(shape = len(grid_values))
        for i in range(len(grid_values)):
            print("In Grid Value {}".format(i))
            y[i] = self.simulate_all_runs(grid_values[i])
            print("Finished grid value {} with final averaged regret {}".format(grid_values[i], y[i]))

        plt.legend()
        plt.plot(grid_values, y)
        plt.title("Bayesian Regret as a function of lambda")
        plt.ylabel('Bayesian Regret')
        plt.xlabel('Lambda')
        plt.show()




    
        
    