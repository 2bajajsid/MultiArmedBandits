import numpy as np
from numpy import random
from game.game import Game

class Partial_Information_Game(Game):
    def __init__(self, data_generating_mechanism, bandit_algorithm, compute_importance_weighted_rewards = False):
        self.store_importance_weighted_rewards = compute_importance_weighted_rewards
        super().__init__(data_generating_mechanism, bandit_algorithm)

    def simulate_one_run(self):
        # initialize the data structures
        # to hold the losses of each arm
        self.data_generating_mechanism.initialize_parameters()
        importance_weighted_history = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                                        self.data_generating_mechanism.get_T()))
        
        unweighted_history = []
        full_history = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                         self.data_generating_mechanism.get_T()))
        arm_chosen = np.zeros(shape = self.data_generating_mechanism.get_T())
        regret = np.zeros(shape = self.data_generating_mechanism.get_T())

        for i in range(self.data_generating_mechanism.get_K()):
            unweighted_history.append([])

        for t in range(self.data_generating_mechanism.get_T()):
            # estimate arm values 
            I_t = self.bandit_algorithm.get_arm_to_pull(importance_weighted_history, unweighted_history, 
                                                        t, self.store_importance_weighted_rewards)

            # reward is  
            # generated by
            # nature
            r_t = self.bandit_algorithm.data_generating_mechanism.get_rewards(t)
            if (self.bandit_algorithm.data_generating_mechanism.mustUpdateStatistics):
                self.bandit_algorithm.data_generating_mechanism.update_statistics(I_t, r_t[I_t], t)

            # update the statistics
            # only add non-zero value to the history
            # if arm was chosen by the learner
            # to simulate partial info
            for i in range(self.data_generating_mechanism.get_K()): 
                if (i == I_t):
                    unweighted_history[i].append(r_t[i])
                    arm_chosen[t] = int(i)
                else:
                    importance_weighted_history[i][t] = 0
                full_history[i][t] = r_t[i]
            
        cumulative_rewards = np.sum(full_history, axis=1)
        optimal_arm_mean = self.bandit_algorithm.data_generating_mechanism.get_optimal_arm_mean()

        for t in range(self.bandit_algorithm.data_generating_mechanism.get_T()):
            if (t == 0):
                regret[t] = optimal_arm_mean - self.bandit_algorithm.data_generating_mechanism.get_arm_mean(arm_chosen[t])
            else: 
                regret[t] = regret[t-1] + (optimal_arm_mean - self.bandit_algorithm.data_generating_mechanism.get_arm_mean(arm_chosen[t]))

        return regret
    


    
        
    