import numpy as np
from numpy import random
from game.game import Game

class Partial_Information_Game(Game):
    def __init__(self, data_generating_mechanism, bandit_algorithm, compute_importance_weighted_rewards = False):
        self.store_importance_weighted_rewards = compute_importance_weighted_rewards
        super().__init__(data_generating_mechanism, bandit_algorithm)

    def simulate_one_run(self):
        # initialize the data structures
        # to hold the losses of each arm
        importance_weighted_history = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                                        self.data_generating_mechanism.get_T()))
        
        unweighted_history = []

        full_history = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                         self.data_generating_mechanism.get_T()))
        
        P_ti = np.zeros(shape = (self.data_generating_mechanism.get_K(), 
                                 self.data_generating_mechanism.get_T()))
        
        regret = np.zeros(shape = self.data_generating_mechanism.get_T())

        for i in range(self.data_generating_mechanism.get_K()):
            unweighted_history.append([])

        for t in range(self.data_generating_mechanism.get_T()):
            # estimate arm values 
            I_t = self.bandit_algorithm.get_arm_to_pull(importance_weighted_history, unweighted_history, t, self.store_importance_weighted_rewards)
            P_ti[:, t] = self.bandit_algorithm.current_sampling_distribution

            # reward is  
            # generated by
            # nature
            r_t = self.data_generating_mechanism.get_rewards(t)

            # update the statistics
            # only add non-zero value to the history
            # if arm was chosen by the learner
            # to simulate partial info
            for i in range(self.data_generating_mechanism.get_K()): 
                if (i == I_t):
                    importance_weighted_history[i][t] = (r_t[i] / P_ti[i][t]) if self.store_importance_weighted_rewards else ((1 - r_t[i]) / P_ti[i][t])
                    unweighted_history[i].append(1 - r_t[i])
                else:
                    importance_weighted_history[i][t] = 0
                full_history[i][t] = 1 - r_t[i]
            
        cumulative_losses = np.sum(full_history, axis=1)
        optimal_arm = np.argmin(cumulative_losses)

        for t in range(self.data_generating_mechanism.get_T()):
            if (t == 0):
                regret[t] = (np.dot(P_ti[:, t], full_history[:, t])) - full_history[optimal_arm][t]
            else: 
                regret[t] = regret[t-1] + (np.dot(P_ti[:, t], full_history[:, t]) - full_history[optimal_arm][t])

        return regret
    


    
        
    